---
title: "Continuous Probability Distributions"
author: "Ben Temperton"
date: "06/11/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(cowplot)
library(colorblindr)
cols <- c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#999999")
```

Here, we describe the distributions found when your data is **continuous**

## The Uniform distribution

A uniform distribution is the simplest of continuous distribution, where one where all values between a minimum, $a$ and a maximum $b$ are equally likely, where:

$$
-\infty \lt a,b \lt \infty
$$

```{r, echo=FALSE}
df = tibble(x=c(runif(10000, min=-4, max=4), 
                runif(10000, min=100, max=1000), 
                runif(10000, min=0, max=1)), 
            y=c(rep('min=-4, max=4', 10000), rep('min=100, max=1000', 10000), rep('min=0, max=1', 10000)))
df$y<-as.factor(df$y)
ggplot(df, aes(x=x, fill=y)) + 
    geom_density() + 
    facet_wrap(~y, nrow=1, scale='free') +
    theme_minimal_hgrid(12) +
    scale_x_continuous(name="Value") +
    scale_y_continuous(name='Count') +
    scale_fill_manual(
    values = cols
  ) +
  theme(
    axis.line = element_blank(),
    strip.text = element_text(size = 12, margin = margin(0, 0, 6, 0, "pt")),
    legend.position = "none"
  )
    
```

Note there is some bumpiness in the tops of these distributions - this is because the figures are generated using 10,000 random numbers picked from a uniform distribution using the `rpois` function. The more random numbers you pick, the less bumpy the tops become (because you have more observations):

```{r, echo=FALSE}
df = tibble(x=c(runif(10, min=-4, max=4),
                runif(100, min=-4, max=4), 
                runif(10000, min=-4, max=4), 
                runif(1e6, min=-4, max=4)), 
            y=c(rep('10 observations', 10), rep('100 observations', 100), rep('10,000 observations', 10000), rep('1,000,000 observations', 1e6)))
df$y<-factor(df$y, levels=c('10 observations', '100 observations', '10,000 observations', '1,000,000 observations'))
ggplot(df, aes(x=x, fill=y)) + 
    geom_density() + 
    facet_wrap(~y, nrow=2) +
    theme_minimal_hgrid(12) +
    scale_x_continuous(name="Value", limits=c(-10,10)) +
    scale_y_continuous(name='Count') +
    scale_fill_manual(
    values = cols
  ) +
  theme(
    axis.line = element_blank(),
    strip.text = element_text(size = 12, margin = margin(0, 0, 6, 0, "pt")),
    legend.position = "none"
  )
    
```

You'll also notice that the slopes of the distribution get straighter the more observations you make. This provides a warning on how one should interpret density plots with few observations - none of the observations are less than $a$ or greater than $b$, but the density plot is smoothing out the curve from the baseline, making it look like there is a low probability of finding values beyond these extremes. If we plot them with a histogram, these go away:

```{r, echo=FALSE}
df = tibble(x=c(runif(10, min=-4, max=4),
                runif(100, min=-4, max=4), 
                runif(10000, min=-4, max=4), 
                runif(1e6, min=-4, max=4)), 
            y=c(rep('10 observations', 10), rep('100 observations', 100), rep('10,000 observations', 10000), rep('1,000,000 observations', 1e6)))
df$y<-factor(df$y, levels=c('10 observations', '100 observations', '10,000 observations', '1,000,000 observations'))
ggplot(df, aes(x=x, fill=y)) + 
    geom_histogram(bins=100) + 
    facet_wrap(~y, nrow=2, scale='free_y') +
    theme_minimal_hgrid(12) +
    scale_x_continuous(name="Value", limits=c(-10,10)) +
    scale_y_continuous(name='Count') +
    scale_fill_manual(
    values = cols
  ) +
  theme(
    axis.line = element_blank(),
    strip.text = element_text(size = 12, margin = margin(0, 0, 6, 0, "pt")),
    legend.position = "none"
  )
    
```

A separate discussion on how best to visualise distributions can be found here INSERT LINK TO APPROPRIATE CHAPTER.

## The Normal, or Gaussian distribution

The normal distribution is probably the distribution that almost everybody has heard of - its use is extremely ubiquitous across all sciences and particularly biology. Its shape is characterised by a symmetric 'bell curve', described by two parameters $\sigma$, which is the standard deviation, and $\mu$, which is the mean:

It has a probability density function with two parameters:
$$
p(x) = \frac{1}{\sigma\sqrt{2\pi}}exp-\frac{(x-\mu)^{2}}{2\sigma^{2}}
$$





### The Central Limit Theorem
The reason the normal distribution is so ubiquitous in science is because of the __central limit theorem__. Imagine you have some population with an underlying distribution - it may be any kind: continuous or discrete, Poisson, exponential, negative binomial _etc._. You make $s$ observations from that population, and then you take the __mean__ of that set of observations. You then repeat this over and over again $n$ times, each time taking $s$ observations and then calculating the mean, so you end up with a series of means:

$$
[\bar{\chi_{1}}, \bar{\chi_{2}}, \bar{\chi_{3}}, \bar{\chi_{4}}, \bar{\chi_{5}} .. \bar{\chi_{n}}]
$$

Regardless of how weird the underlying distribution is, the distribution of those means will approximate a normal distribution as $n$ increases. 

We can see that here, if we repeatedly sample five observations ($s=5$) from a population of 10,000 with an exponential distribution, where $n=10, 100, 1000, 10000$:

```{r, echo=FALSE}
bootstrap_replicate_1d<-function(data, func, size=NULL){
  if (is.null(size)){
    size = length(data)
  }
  bs_sample = sample(data, size=size, replace=TRUE)
  do.call(func, list(x=bs_sample))
}

generate_bootstrap_replicate<-function(data, func, n=10000, size=NULL){
  replicate(n, do.call(bootstrap_replicate_1d, list(data=data, func=func, size=size)))
}
population = rexp(10000)

s5_n10 <- generate_bootstrap_replicate(population, mean, n=10, size =5)
s5_n100 <- generate_bootstrap_replicate(population, mean, n=100, size =5)
s5_n1000 <- generate_bootstrap_replicate(population, mean, n=1000, size =5)
s5_n10000 <- generate_bootstrap_replicate(population, mean, n=10000, size =5)

df = tibble(x=c(s5_n10, s5_n100, s5_n1000, s5_n10000), 
            y=c(rep('s=5, n=10', 10), rep('s=5, n=100', 100), rep('s=5, n=1000', 1000), rep('s=5, n=10000', 10000)))
df$y<-factor(df$y, levels=c('s=5, n=10', 's=5, n=100', 's=5, n=1000', 's=5, n=10000'))
ggplot(df, aes(x=x, fill=y)) + 
    geom_density() +
    facet_wrap(~y, nrow=1) +
    theme_minimal_hgrid(12) +
    scale_x_continuous(name="Mean of n") +
    scale_y_continuous(name='Count') +
    scale_fill_manual(
    values = cols
  ) +
  theme(
    axis.line = element_blank(),
    strip.text = element_text(size = 12, margin = margin(0, 0, 6, 0, "pt")),
    legend.position = "none"
  )
```

Similarly, if we take more observations per repeat (so increase $s$) we see a reduction in variance (a narrowing of the distribution):

```{r, echo=FALSE}

population = rexp(10000)

s5_n10000 <- generate_bootstrap_replicate(population, mean, n=10000, size =5)
s10_n10000 <- generate_bootstrap_replicate(population, mean, n=10000, size =10)
s20_n10000 <- generate_bootstrap_replicate(population, mean, n=10000, size =20)
s30_n10000<- generate_bootstrap_replicate(population, mean, n=10000, size =30)

df = tibble(x=c(s5_n10000, s10_n10000, s20_n10000, s30_n10000), 
            y=c(rep('s=5, n=10000', 10000), rep('s=10, n=10000', 10000), rep('s=20, n=10000', 10000), rep('s=30, n=10000', 10000)))
df$y<-factor(df$y, levels=c('s=5, n=10000', 's=10, n=10000', 's=20, n=10000', 's=30, n=10000'))
ggplot(df, aes(x=x, fill=y)) + 
    geom_density() +
    facet_wrap(~y, nrow=1) +
    theme_minimal_hgrid(12) +
    scale_x_continuous(name="Mean of n") +
    scale_y_continuous(name='Count') +
    scale_fill_manual(
    values = cols
  ) +
  theme(
    axis.line = element_blank(),
    strip.text = element_text(size = 12, margin = margin(0, 0, 6, 0, "pt")),
    legend.position = "none"
  )
```


### So what does this mean for your experiments? 

### But what if you don't have lots of observations and samples?

## The Exponential distribution
Imagine an experiment observing predation rates of foxes. On average, 10 rabbits a day are predated by foxes. The number of rabbits predated by foxes is **Poisson distributed**, because it can't be less than zero and has discrete values. We call that rate, $\lambda$. If you wanted to predict how long it would take for the _first_ rabbit to be eaten on a given day, we need to reach for the **exponential distribution**. The broad definitiion is - if events are Poisson distributed, and occur randomly over time at a rate of $\lambda$, then the time until the first event occuring is described by an exponential distribution. It only has one parameter, $\lambda$, and its probability density function is described as:

$$
f(x) = \lambda e^{-\lambda x}
$$

You can see here how the value changes as the rate increases:

```{r, echo=FALSE}
df = tibble(x=c(rexp(10000, rate =0.5), 
                rexp(10000, rate=1), 
                rexp(10000, rate=1.5)), 
            y=c(rep('rate = 0.5', 10000), rep('rate = 1', 10000), rep('rate = 1.5', 10000)))
df$y<-as.factor(df$y)
ggplot(df, aes(x=x, fill=y)) + 
    geom_density() + 
    facet_wrap(~y, nrow=1) +
    theme_minimal_hgrid(12) +
    scale_x_continuous(name="Time until first event") +
    scale_y_continuous(name='Count') +
    scale_fill_manual(
    values = cols
  ) +
  theme(
    axis.line = element_blank(),
    strip.text = element_text(size = 12, margin = margin(0, 0, 6, 0, "pt")),
    legend.position = "none"
  )
    
```

So, as the rate $\lambda$ increases (i.e as more rabbits get predated per unit time), the less time it takes for the first rabbit to be predated, so the probabilty of the first predation event occuring in the first few hours increases.

## The Gamma distribution

The gamma distribution is essentially a generalisation of the exponential distribution that describes the probability of the waiting time until the $n^{th}$ event, instead of the first event. 

It is a two-parameter distribution where the parameters are traditionally known as _shape_, $\alpha$ and _rate_, $\lambda$. Its probability density function is defined thus:

$$
f(x; \alpha, \lambda) = \frac{\lambda^{\alpha}}{\Gamma(\alpha)}x^{\alpha-1}e^{-\lambda x}\ \ \ \text{for}\ \alpha,\lambda\gt0
$$

As an example, if you observe one predation event every two days (so $\alpha= 0.5$), how long will it take you to collect 100 datapoints (so $\lambda=100$)? What about if you observe 5 predations a day ($\alpha=5$)? 20?


```{r, echo=FALSE}
df = tibble(x=c(rgamma(10000, rate =0.5, shape=100), 
                rgamma(10000, rate=5, shape=100), 
                rgamma(10000, rate=20, shape=100)), 
            y=c(rep('0.5 per day', 10000), rep('5 per day', 10000), rep('20 per day', 10000)))
df$y<-factor(df$y, levels=c('0.5 per day', '5 per day', '20 per day'))
ggplot(df, aes(x=x, fill=y)) + 
    geom_density() + 
    facet_wrap(~y, nrow=1, scales='free') +
    theme_minimal_hgrid(12) +
    scale_x_continuous(name="Days until 100 data points collected") +
    scale_y_continuous(name='Count') +
    scale_fill_manual(
    values = cols
  ) +
  theme(
    axis.line = element_blank(),
    strip.text = element_text(size = 12, margin = margin(0, 0, 6, 0, "pt")),
    legend.position = "none"
  )
    
```

So, at predation rates of 0.5 per day, you'd be looking at around 160-240 days to collect your data. At 5 per day, you'd be looking at maybe 27-24 days. At 20 per day, you'd be waiting no more than 6 days. Given the above distributions, it becomes apparent that the mean of a gamma distribution is ratio of the _shape_ and the _rate_, so 

$$
E[X] = \frac{\alpha}{\lambda}
$$

**Remember that an exponential distribution is simply a gamma distribution where the shape, $\alpha=1$**

